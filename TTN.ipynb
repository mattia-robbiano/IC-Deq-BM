{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module TTN_utilities.\n",
      "WARNING: using TTN_utilities.squared_norm in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using ITensors\n",
    "using CUDA\n",
    "include(\"TTN_utilities.jl\")\n",
    "using .TTN_utilities\n",
    "ITensors.set_warn_order(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#= # Example of building a simple TTN manually\n",
    "nsites = 4\n",
    "sites = [Index(2, \"site $i\") for i in 1:nsites] =#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ITensors when we define an index, we are implicitly defining a dimension for a generic vector space. In this case we are definig 4 bidimensional spaces (for example each one can be associated with a spin).\n",
    "\n",
    "We can use indexes to define and work with tensors. For example defining: \n",
    "\n",
    "-- ITensor(index(2,a)) creates a rank-1 tensor of dimension 2, so a vector of lenght 2.\n",
    "\n",
    "-- ITensor(index(2,a), index*(2,a)) creates a 2x2 matrix, so a transformation between the two 2d spaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contraction is a sum over indexes with same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#= # Leaf tensors\n",
    "leaf1 = randomITensor(sites[1], sites[2])\n",
    "leaf2 = randomITensor(sites[3], sites[4])\n",
    "\n",
    "# Parent tensor\n",
    "parent = randomITensor(sites[1], sites[2], sites[3], sites[4])\n",
    "\n",
    "# contracting\n",
    "result = parent * leaf1 * leaf2\n",
    "\n",
    "println(\"Result of contracting the TTN with random vectors: \", result) =#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#= ABC = @visualize leaf1 * leaf2 * parent edge_labels=(tags=true,); =#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contracting parent, a ITensor(i1,i2,i3,i4) with the two leaves, ITensor(i1,i2) and ITensor(i3,i4) returns a scalar since we are summing over all indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-TensorNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.155131"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/diagram_TTN.png\" alt=\"Example Image\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated in Fig. 1(b), each circle represents\n",
    "a tensor; each edge of the circle represents an individual\n",
    "index of the tensor. The first tensor is a matrix connecting the\n",
    "second and third tensors, while the remaining tensors are all\n",
    "three-order tensors with three indices. The index between two\n",
    "tensors is called a virtual bond, which would be contracted\n",
    "hereafter. The left and right indices of the tensors in the\n",
    "bottom of the TTN are respectively connected to two pixels\n",
    "of the input image and hence are called physical bonds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bond dimension in a tensor network refers to the dimension of the shared index (or bond) connecting two tensors. \n",
    "Higher bond dimensions allow more complex entanglement between subsystems to be represented. In quantum many-body systems, the bond dimension represents the number of states used to approximate the entanglement structure.\n",
    "\n",
    "Since bond dimension represents on how many indices we are summing, it is related to how well we can represent states with our tensor network.\n",
    "In the context of Matrix Product States (MPS), the bond dimension directly corresponds to the Schmidt rank of the state for each bipartition. If the bond dimension is h , the MPS can capture states with a maximum Schmidt rank of h.\n",
    "\n",
    "$|\\psi\\rangle=\\sum_{i=1}^D \\lambda_i\\left|u_i\\right\\rangle_A \\otimes\\left|v_i\\right\\rangle_B$, where D is the Schmidt rank, the number of non-zero $\\lambda$, which quantifies the enganglement between the two subsystems A and B (for D=1 the state is separable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{ITensor}:\n",
       " ITensor ord=2\n",
       "Dim 1: (dim=2|id=813|\"bound21\")\n",
       "Dim 2: (dim=2|id=237|\"bound22\")\n",
       "NDTensors.Dense{Float64, Vector{Float64}}\n",
       " 2Ã—2\n",
       "  0.7078771135698944  -1.042245894005896\n",
       " -0.5248477493998434  -1.5737037049256306"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters\n",
    "# Last layer contain only root tensor\n",
    "number_physical_indexes = 8\n",
    "bond_dimension = 2\n",
    "number_layers = 3\n",
    "\n",
    "# Define indexes\n",
    "number_bound_1_indexes = div(number_physical_indexes,2)\n",
    "number_bound_2_indexes = div(number_physical_indexes,4)\n",
    "\n",
    "index_physical = [Index(2, \"physical $i\") for i in 1:number_physical_indexes]\n",
    "index_bound_1 = [Index(bond_dimension, \"bound 1 $i\") for i in 1:number_bound_1_indexes]\n",
    "index_bound_2 = [Index(bond_dimension, \"bound 2 $i\") for i in 1:number_bound_2_indexes]\n",
    "\n",
    "# Build TTN\n",
    "TTN = Vector{Vector{ITensor}}(undef, number_layers)\n",
    "TTN[1] = [randomITensor(index_physical[2*i-1], index_physical[2*i], index_bound_1[i]) for i in 1:number_bound_1_indexes]\n",
    "TTN[2] = [randomITensor(index_bound_1[2*i-1], index_bound_1[2*i], index_bound_2[i]) for i in 1:number_bound_2_indexes]\n",
    "TTN[3] = [randomITensor(index_bound_2[1], index_bound_2[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTN converted to CUDA tensors\n"
     ]
    }
   ],
   "source": [
    "# Converting tensors to CUDA tensors\n",
    "v1 = TTN[1]\n",
    "v2 = TTN[2]\n",
    "v3 = TTN[3]\n",
    "v1 = [ cu(t) for t in v1 ]\n",
    "v2 = [ cu(t) for t in v2 ]\n",
    "v3 = [ cu(t) for t in v3 ]\n",
    "TTN[1] = v1\n",
    "TTN[2] = v2\n",
    "TTN[3] = v3\n",
    "println(\"TTN converted to CUDA tensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "548.6406609072357"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calcluating norm at this stage with different contraction orders for future observation\n",
    "\n",
    "contracted_initial_bottom = (TTN[1][1]*TTN[2][1]*TTN[1][2])*TTN[3][1]*(TTN[1][3]*TTN[2][2]*TTN[1][4])\n",
    "norm_initial_bottom = sqrt((contracted_initial_bottom*dag(contracted_initial_bottom))[][])\n",
    "\n",
    "contracted_initial_top = TTN[3][1]*TTN[2][1]*TTN[2][2]*TTN[1][1]*TTN[1][2]*TTN[1][3]*TTN[1][4]\n",
    "norm_initial_top = sqrt((contracted_initial_top*dag(contracted_initial_top))[][])\n",
    "\n",
    "tensors_ordered_array = vcat(TTN[1],TTN[2],TTN[3])\n",
    "conj_tensors_ordered_array = [dag(t) for t in tensors_ordered_array]\n",
    "norm_ordered_squared = 1.0\n",
    "for (tensors_ordered_array, conj_tensors_ordered_array) in zip(tensors_ordered_array, conj_tensors_ordered_array)\n",
    "    norm_ordered_squared *= tensors_ordered_array * conj_tensors_ordered_array\n",
    "end\n",
    "norm_initial_ordered = sqrt(norm_ordered_squared[][])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonicalization basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify things, we need to bring our tensors in canonical form, in which each tensor is in canonical form for an index (in following case for index $\\alpha_2$)\n",
    "$$\n",
    "\\sum_{\\alpha_4, \\alpha_5} T_{\\alpha_2, \\alpha_4, \\alpha_5}^{[2]} T_{\\alpha_2^{\\prime}, \\alpha_4, \\alpha_5}^{[2]}=\\delta_{\\alpha_2, \\alpha_2^{\\prime}},\n",
    "$$\n",
    "\n",
    "We can do so, by using QR decomposition. Starting from the leaves, we can decompose each tensor in a Unitary part and a Residual part. Then propagating the residual part towards the root we can for a \"central tensor\" containing all non-canonical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#= # QR decomposition of a tensor as example\n",
    "# Example of building a simple TTN manually\n",
    "nsites = 3\n",
    "sites = [Index(2, \"site $i\") for i in 1:nsites]\n",
    "\n",
    "A = randomITensor(sites[1], sites[2], sites[3])\n",
    "A = A / norm(A)\n",
    "Q, R = qr(A, [sites[1], sites[2]], [sites[3]])\n",
    "println(\"norma di Q: \", norm(Q))\n",
    "println(\"norma di R: \", norm(R)) =#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#= A_reconstructed = Q * R\n",
    "println(\"Reconstructed tensor: \", A_reconstructed)\n",
    "println(\"Reconstruction error: \", norm(A - A_reconstructed))\n",
    " =#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#= # Check that Q is unitary\n",
    "# Normalize Q\n",
    "Q = Q\n",
    "Q_dagger = dag(Q)\n",
    "result = Q * Q_dagger\n",
    "println(\"Q * Q_dagger: \", result) =#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonicalization of TTN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now procede in canonicalization procedure for each layer of the TNN, taking the root tensor as central tensor. In this procedure we have to keep an eye on indexes, since the QR decomposition, the first block of indices is assigned to Q and the second one (in our case, the only \"upper\" one) is assigned to R, to be contracted with the next layer. The algorithm creates a new set of indexes beetween the first and second layer, that we have to rename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting reduction to canonical form\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ITensor ord=2 (dim=2|id=813|\"bound21\") (dim=2|id=237|\"bound22\")\n",
       "NDTensors.Dense{Float32, CuArray{Float32, 1, CUDA.DeviceMemory}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(\"Starting reduction to canonical form\")\n",
    "\n",
    "# first layer\n",
    "for i in 1:number_bound_1_indexes\n",
    "    Q, R = qr(TTN[1][i], [index_physical[2*i-1],index_physical[2*i]], [index_bound_1[i]])\n",
    "    norm_Q = sqrt((Q*dag(Q))[][])\n",
    "    R = R*norm_Q\n",
    "    Q = Q/norm_Q\n",
    "    TTN[1][i] = Q\n",
    "    TTN[2][Int(ceil(i/2))] = R*TTN[2][Int(ceil(i/2))]\n",
    "end\n",
    "\n",
    "# restoring indexes names\n",
    "for i in 1:number_bound_1_indexes\n",
    "    replaceind!(TTN[1][i], inds(TTN[1][i])[3], index_bound_1[i])\n",
    "end\n",
    "for i in 1:number_bound_2_indexes\n",
    "    replaceind!(TTN[2][i], inds(TTN[2][i])[1], index_bound_1[2*i-1])\n",
    "    replaceind!(TTN[2][i], inds(TTN[2][i])[2], index_bound_1[2*i])\n",
    "end\n",
    "\n",
    "# second layer\n",
    "for i in 1:number_bound_2_indexes\n",
    "    Q, R = qr(TTN[2][i], [index_bound_1[2*i-1],index_bound_1[2*i]], [index_bound_2[i]])\n",
    "    norm_Q = sqrt((Q*dag(Q))[][])\n",
    "    R = R*norm_Q\n",
    "    Q = Q/norm_Q\n",
    "    TTN[2][i] = Q\n",
    "    TTN[3][1] = R * TTN[3][1]\n",
    "end\n",
    "\n",
    "# restoring indexes names\n",
    "for i in 1:number_bound_2_indexes\n",
    "    replaceind!(TTN[2][i], inds(TTN[2][i])[3], index_bound_2[i])\n",
    "end\n",
    "replaceind!(TTN[3][1], inds(TTN[3][1])[1], index_bound_2[1])\n",
    "replaceind!(TTN[3][1], inds(TTN[3][1])[2], index_bound_2[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the squared norm of the entire TTN making it contract with its conjugate (function defined in TTN_utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.375807f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calcluating norm at this stage with different contraction orders for future observation\n",
    "\n",
    "contracted_final_bottom = (TTN[1][1]*TTN[2][1]*TTN[1][2])*TTN[3][1]*(TTN[1][3]*TTN[2][2]*TTN[1][4])\n",
    "norm_final_bottom = sqrt((contracted_final_bottom*dag(contracted_final_bottom))[][])\n",
    "\n",
    "contracted_final_top = TTN[3][1]*TTN[2][1]*TTN[2][2]*TTN[1][1]*TTN[1][2]*TTN[1][3]*TTN[1][4]\n",
    "norm_final_top = sqrt((contracted_final_top*dag(contracted_final_top))[][])\n",
    "\n",
    "norm_final_root = sqrt((TTN[3][1]*dag(TTN[3][1]))[][])\n",
    "\n",
    "tensors_ordered_array = vcat(TTN[1],TTN[2],TTN[3])\n",
    "conj_tensors_ordered_array = [dag(t) for t in tensors_ordered_array]\n",
    "norm_ordered_squared = 1.0\n",
    "for (tensors_ordered_array, conj_tensors_ordered_array) in zip(tensors_ordered_array, conj_tensors_ordered_array)\n",
    "    norm_ordered_squared *= tensors_ordered_array * conj_tensors_ordered_array\n",
    "end\n",
    "norm_final_ordered = sqrt(norm_ordered_squared[][])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation about contraction order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we contract the tensor network bottom up (or top bottom, same result) into a singe tensor and with its conjugate, to calculate norm, before the transformation to canonical form we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_initial_bottom: 28.375814\n",
      "norm_initial_top: 28.375813\n"
     ]
    }
   ],
   "source": [
    "println(\"norm_initial_bottom: \",norm_initial_bottom)\n",
    "println(\"norm_initial_top: \",norm_initial_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_final_bottom: 28.375809\n",
      "norm_final_top: 28.375807\n"
     ]
    }
   ],
   "source": [
    "println(\"norm_final_bottom: \",norm_final_bottom)\n",
    "println(\"norm_final_top: \",norm_final_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quantity is then conserved by the transformation. This is what we were expecting, since we are contracting back the $Q_i$ tensors back in the root contracted with all the $R_i$, finding again the initial tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we contract each tensor with the respective conjugate before and after the transformation, we get another scalar value, that is not conserved by the procedure. However, after the transformation this value equals to the norm of the root tensor, and this is what we were expecting since each node different from the root is a unitary matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_initial_ordered: 548.6406609072357\n",
      "norm_final_ordered: 227.00647693742675\n",
      "norm_final_root: 227.00647\n"
     ]
    }
   ],
   "source": [
    "println(\"norm_initial_ordered: \",norm_initial_ordered)\n",
    "println(\"norm_final_ordered: \",norm_final_ordered)\n",
    "println(\"norm_final_root: \",norm_final_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
