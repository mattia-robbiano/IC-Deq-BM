{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-math in ./.venv/lib/python3.11/site-packages (0.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import quimb as qu\n",
    "import quimb.tensor as qtn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-TensorNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.155131"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/diagram_TTN.png\" alt=\"Example Image\" width=\"1000\" style=\"display: block; margin-left: auto; margin-right: auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated in Fig. 1(b), each circle represents\n",
    "a tensor; each edge of the circle represents an individual\n",
    "index of the tensor. The first tensor is a matrix connecting the\n",
    "second and third tensors, while the remaining tensors are all\n",
    "three-order tensors with three indices. The index between two\n",
    "tensors is called a virtual bond, which would be contracted\n",
    "hereafter. The left and right indices of the tensors in the\n",
    "bottom of the TTN are respectively connected to two pixels\n",
    "of the input image and hence are called physical bonds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bond dimension in a tensor network refers to the dimension of the shared index (or bond) connecting two tensors. \n",
    "Higher bond dimensions allow more complex entanglement between subsystems to be represented. In quantum many-body systems, the bond dimension represents the number of states used to approximate the entanglement structure.\n",
    "\n",
    "Since bond dimension represents on how many indices we are summing, it is related to how well we can represent states with our tensor network.\n",
    "In the context of Matrix Product States (MPS), the bond dimension directly corresponds to the Schmidt rank of the state for each bipartition. If the bond dimension is h , the MPS can capture states with a maximum Schmidt rank of h.\n",
    "\n",
    "$|\\psi\\rangle=\\sum_{i=1}^D \\lambda_i\\left|u_i\\right\\rangle_A \\otimes\\left|v_i\\right\\rangle_B$, where D is the Schmidt rank, the number of non-zero $\\lambda$, which quantifies the enganglement between the two subsystems A and B (for D=1 the state is separable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ttn(number_physical_indexes, bond_dimension, number_layers):\n",
    "    #\n",
    "    # this function builds a tree tensor network with the specified number of physical indexes, bond dimension and number of layers\n",
    "    # the tree tensor network is composed of number_layers layers of rank-3 random tensors halving the number at each layer\n",
    "    # the function returns the tensor network, a matrix of tensors composing it and the list of lenghts of each layer\n",
    "    # root is considered a layer and the function does not check if the number of physical indexes is compatible with the number of layers\n",
    "    #\n",
    "    ttn_shape = [int(number_physical_indexes / (2 ** (i + 1))) for i in range(number_layers)]\n",
    "    \n",
    "    index = [[] for _ in range(number_layers)]\n",
    "    index[0] = [f'ph{i}' for i in range(number_physical_indexes)]\n",
    "    for i in range(1, number_layers):\n",
    "        index[i] = [f'b{i-1}{j}' for j in range(ttn_shape[i-1])]\n",
    "    \n",
    "    tensor = [[] for _ in range(number_layers)]\n",
    "    tensor[0] = [qtn.rand_tensor([bond_dimension] * 3, inds=[index[0][2 * i], index[0][2 * i + 1], index[1][i]], tags=f't0{i}') for i in range(ttn_shape[0])]\n",
    "    for i in range(1, number_layers - 1):\n",
    "        tensor[i] = [qtn.rand_tensor([bond_dimension] * 3, inds=[index[i][2 * j], index[i][2 * j + 1], index[i + 1][j]], tags=f't{i}{j}') for j in range(ttn_shape[i])]\n",
    "    tensor[number_layers - 1] = [qtn.rand_tensor([bond_dimension] * 2, inds=[index[number_layers - 1][2 * k], index[number_layers - 1][2 * k + 1]], tags=f't{number_layers - 1}{k}') for k in range(ttn_shape[number_layers - 1])]\n",
    "    \n",
    "    return qtn.TensorNetwork(tensor), tensor, ttn_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_physical_indexes = 16\n",
    "bond_dimension = 2\n",
    "number_layers = 4\n",
    "\n",
    "tensor_network, tensor, ttn_shape = build_ttn(number_physical_indexes, bond_dimension, number_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can access tags of tensors with tensor_map\n",
    "#  for name, specs in tensor_network.tensor_map.items():\n",
    "#     print(f\"Tensor name: {name}\")\n",
    "#     print(f\"Description: {tensor}\")\n",
    "#     print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_network.draw(show_inds=True,show_tags=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can contract the whole tensor network and calculate the forbenius norm of the resulting rank-8 tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contracted = tensor_network.contract(all, optimize='auto-hq')\n",
    "# contracted.draw(color=['t0','t1','t2'],show_inds=True,show_tags=True)\n",
    "# print('norm: ',contracted.norm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now procede in canonicalization procedure for each layer of the TNN, taking the root tensor as central tensor. In this procedure we have to keep an eye on indexes, since the QR decomposition, the first block of indices is assigned to Q and the second one (in our case, the only \"upper\" one) is assigned to R, to be contracted with the next layer. The algorithm creates a new set of indexes beetween the first and second layer, that we have to rename."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/canonize-bond.png\" alt=\"Example Image\" width=\"1500\" style=\"display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "    <p>The arrow on U means it's unitary</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quimb has a default algorith to canonize bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(ttn_shape[1]):\n",
    "    qtn.tensor_canonize_bond(tensor[0][2*i], tensor[1][i])\n",
    "    qtn.tensor_canonize_bond(tensor[0][2*i+1], tensor[1][i])\n",
    "for i in range(ttn_shape[2]):\n",
    "    qtn.tensor_canonize_bond(tensor[1][2*i], tensor[2][i])\n",
    "    qtn.tensor_canonize_bond(tensor[1][2*i+1], tensor[2][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('root norm: ',tensor[2][0].norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contracted = tensor_network.contract(all, optimize='auto-hq')\n",
    "# contracted.draw(color=['t0','t1','t2'],show_inds=True,show_tags=True)\n",
    "# print(contracted.norm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canonization preserves Forbenius norm and the norm of the whole tensor network is given by the norm of the root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfect sampling algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tensornetwork.org/mps/algorithms/sampling/ here the example is aplied to MPS but can be generalized to other TNs, including TTN\n",
    "\n",
    "https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.155131 chapter II-F\n",
    "\n",
    "https://journals.aps.org/prb/pdf/10.1103/PhysRevB.85.165146\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assume our probability distribution to be:\n",
    "$$p\\left(s_1, s_2, s_3, \\ldots, s_N\\right)=\\left|T^{s_1 s_2 s_3 \\cdots s_N}\\right|^2$$\n",
    "First we need to make sure it is normalized to one.\n",
    "$$\\sum_{\\{s\\}}\\left|T^{s_1 s_2 s_3 \\cdots s_N}\\right|^2=1 \\text {, or }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_network = tensor_network / tensor_network.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principle underlying the MPS sampling algorithm below is the “chain rule” of probability\n",
    "\n",
    "$$p\\left(s_1, s_2, s_3, \\ldots, s_N\\right)=p\\left(s_1\\right) p\\left(s_2 \\mid s_1\\right) p\\left(s_3 \\mid s_1 s_2\\right) \\cdots p\\left(s_N \\mid s_1 s_2 s_3 \\cdots s_{N-1})\\right.$$\n",
    "\n",
    "computing each factor on the right we will get a single sample from the full distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let' start with $P(s_1)$\n",
    "$$p\\left(s_1\\right)=\\sum_{s_2, s_3, \\ldots, s_N} p\\left(s_1, s_2, \\ldots, s_N\\right)=\\sum_{s_2, s_3, \\ldots, s_N} T^{s_1 s_2 \\cdots s_N} \\bar{T}^{s_1 s_2 \\ldots s_N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/image3.png\" alt=\"Sampling Algorithm Image\" width=\"600\" style=\"display: block; margin-left: auto; margin-right: auto;\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_tensor_network = tensor_network.H.reindex({'ph0':'ph0*'})&tensor_network\n",
    "# step_tensor_network.draw(color=['t00'],show_tags=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_tensor_network = step_tensor_network.contract(all)\n",
    "# reduced_tensor_network.draw(color=['t00'],show_inds=True,show_tags=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a matrix (“reduced density matrix”), but here we will only need its diagonal elements, ph0=ph0*. \n",
    "Contracting with the basis elements (0,1) and (1,0) we get $p(s_0)$ and $p(s_1)$. It must hold $p(s_0)+p(s_1)=1$. Drawing a value from this probability distribution is equivalent to extract a value in $r\\in[0,1]$ and if $r<p_1$ then $\\hat{s_1}=(0,1)$ and viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0 = qtn.Tensor(data=[0,1], inds=['ph0'])\n",
    "# v1 = qtn.Tensor(data=[1,0], inds=['ph0'])\n",
    "# ps0 = v0@reduced_tensor_network@v0.reindex({'ph0':'ph0*'})\n",
    "# ps1 = v1@reduced_tensor_network@v1.reindex({'ph0':'ph0*'})\n",
    "# print('ps0: ',ps0)\n",
    "# print('ps1: ',ps1)\n",
    "# print('normalization check: ',ps0+ps1)\n",
    "# r = np.random.uniform(0, 1)\n",
    "# print('random number: ', r)\n",
    "# s_hat = [[]]\n",
    "# if r < ps0:\n",
    "#     s_hat[0] = v0.data\n",
    "# else:\n",
    "#     s_hat[0] = v1.data\n",
    "# print('s_hat vector: ', s_hat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can contract the tensor network with s0 and repeat the process for $s_2$, to compute $p(s_2|\\hat s_1)$, and so on to $p(\\hat s_N,....,\\hat s_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = qtn.Tensor(data=s_hat[0], inds=['ph0'])\n",
    "# step_tensor_network = step_tensor_network @ v.reindex({'ph0':'ph0*'}) @ v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will write it down again in one algorithm for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_network = tensor_network / tensor_network.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perfect_sampling_ttn(ttn,number_indexes):\n",
    "    s_hat = np.zeros([number_indexes,2])\n",
    "    probability = np.zeros(number_indexes)\n",
    "\n",
    "    # cycle 0: the ttn is connected to the conjugate, except for the ph0 index, we obtain a matrix, multiplied by the basis vectors\n",
    "    # gives me the probability of extracting the first element. We extract the first element\n",
    "    # cycle 1: the vector extracted in the previous cycle is connected to the ph0 index of the ttn. The procedure repeats identically\n",
    "\n",
    "    for i in range(number_indexes):\n",
    "        step_tensor_network = tensor_network\n",
    "\n",
    "        for j in range(i): \n",
    "            # for each cycle connect a tensor to an index (in sequence from ph0 to ph14) up to the index before \n",
    "            # the one I want to sample\n",
    "            v = qtn.Tensor(data=s_hat[j], inds=[f'ph{j}'], tags=[f'v{int(s_hat[j][0])}'])\n",
    "            step_tensor_network = step_tensor_network & v\n",
    "            step_tensor_network = step_tensor_network / np.sqrt(probability[j])\n",
    "\n",
    "        # take the complex conjugate of the new network with the same indices as the first one except for the one I want to sample\n",
    "        step_tensor_network_full = step_tensor_network.H.reindex({f'ph{i}':f'ph{i}*'}) & step_tensor_network\n",
    "\n",
    "        # contraction of the network, I get the probability matrix of extracting the new element\n",
    "        reduced_tensor_network = step_tensor_network_full.contract(all)\n",
    "\n",
    "        # calculate the probability of extracting the two elements\n",
    "        v0 = qtn.Tensor(data=[0,1], inds=[f'ph{i}'])\n",
    "        v1 = qtn.Tensor(data=[1,0], inds=[f'ph{i}'])\n",
    "        ps0 = v0 @ reduced_tensor_network @ v0.reindex({f'ph{i}':f'ph{i}*'})\n",
    "        ps1 = v1 @ reduced_tensor_network @ v1.reindex({f'ph{i}':f'ph{i}*'})\n",
    "\n",
    "        ##########NORMALIZATION TEST############\n",
    "        if ps0+ps1<0.999 or ps0+ps1>1.001:\n",
    "            print(\"errore al ciclo: \",i)\n",
    "            print('ps0: ',ps0)\n",
    "            print('ps1: ',ps1)\n",
    "        ########################################\n",
    "\n",
    "        #extracting new element\n",
    "        r = np.random.uniform(0, 1)\n",
    "        if r < ps0:\n",
    "            s_hat[i] = v0.data\n",
    "            probability[i] = ps0\n",
    "        else:\n",
    "            s_hat[i] = v1.data\n",
    "            probability[i] = ps1\n",
    "    return s_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a frequency histogram sampling the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "samples = []\n",
    "for _ in range(1000):\n",
    "    sample = perfect_sampling_ttn(tensor_network, number_physical_indexes)\n",
    "    samples.append(tuple(sample.flatten()))\n",
    "\n",
    "# Count the occurrences of each unique sample\n",
    "sample_counts = Counter(samples)\n",
    "\n",
    "# Create histogram\n",
    "labels, values = zip(*sample_counts.items())\n",
    "indexes = range(len(labels))\n",
    "\n",
    "plt.figure(figsize=(15, 8))  # Increase figure size\n",
    "plt.bar(indexes, values, tick_label=[\"\".join(map(str, map(int, label))) for label in labels])\n",
    "plt.xlabel('Sample Combinations')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Sample Combinations')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels\n",
    "plt.tight_layout()  # Adjust layout to fit everything\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
