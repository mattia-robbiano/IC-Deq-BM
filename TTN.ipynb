{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ITensors\n",
    "using CUDA\n",
    "include(\"TTN_utilities.jl\")\n",
    "using .TTN_utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#= # Example of building a simple TTN manually\n",
    "nsites = 4\n",
    "sites = [Index(2, \"site $i\") for i in 1:nsites] =#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ITensors when we define an index, we are implicitly defining a dimension for a generic vector space. In this case we are definig 4 bidimensional spaces (for example each one can be associated with a spin).\n",
    "\n",
    "We can use indexes to define and work with tensors. For example defining: \n",
    "\n",
    "-- ITensor(index(2,a)) creates a rank-1 tensor of dimension 2, so a vector of lenght 2.\n",
    "\n",
    "-- ITensor(index(2,a), index*(2,a)) creates a 2x2 matrix, so a transformation between the two 2d spaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contraction is a sum over indexes with same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#= # Leaf tensors\n",
    "leaf1 = randomITensor(sites[1], sites[2])\n",
    "leaf2 = randomITensor(sites[3], sites[4])\n",
    "\n",
    "# Parent tensor\n",
    "parent = randomITensor(sites[1], sites[2], sites[3], sites[4])\n",
    "\n",
    "# contracting\n",
    "result = parent * leaf1 * leaf2\n",
    "\n",
    "println(\"Result of contracting the TTN with random vectors: \", result) =#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#= ABC = @visualize leaf1 * leaf2 * parent edge_labels=(tags=true,); =#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contracting parent, a ITensor(i1,i2,i3,i4) with the two leaves, ITensor(i1,i2) and ITensor(i3,i4) returns a scalar since we are summing over all indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-TensorNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.155131"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/diagram_TTN.png\" alt=\"Example Image\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated in Fig. 1(b), each circle represents\n",
    "a tensor; each edge of the circle represents an individual\n",
    "index of the tensor. The first tensor is a matrix connecting the\n",
    "second and third tensors, while the remaining tensors are all\n",
    "three-order tensors with three indices. The index between two\n",
    "tensors is called a virtual bond, which would be contracted\n",
    "hereafter. The left and right indices of the tensors in the\n",
    "bottom of the TTN are respectively connected to two pixels\n",
    "of the input image and hence are called physical bonds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bond dimension in a tensor network refers to the dimension of the shared index (or bond) connecting two tensors. \n",
    "Higher bond dimensions allow more complex entanglement between subsystems to be represented. In quantum many-body systems, the bond dimension represents the number of states used to approximate the entanglement structure.\n",
    "\n",
    "Since bond dimension represents on how many indices we are summing, it is related to how well we can represent states with our tensor network.\n",
    "In the context of Matrix Product States (MPS), the bond dimension directly corresponds to the Schmidt rank of the state for each bipartition. If the bond dimension is h , the MPS can capture states with a maximum Schmidt rank of h.\n",
    "\n",
    "$|\\psi\\rangle=\\sum_{i=1}^D \\lambda_i\\left|u_i\\right\\rangle_A \\otimes\\left|v_i\\right\\rangle_B$, where D is the Schmidt rank, the number of non-zero $\\lambda$, which quantifies the enganglement between the two subsystems A and B (for D=1 the state is separable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ITensor ord=2 (dim=2|id=977|\"layer2_bond1\") (dim=2|id=431|\"layer2_bond2\")\n",
       "NDTensors.Dense{Float64, Vector{Float64}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's make a TTN with 3 layers, consisting of rank-3 tensors with bond dimension 2 with random elements\n",
    "\n",
    "# Define physical indices\n",
    "n_physical_indexes = 8\n",
    "physical_indexes = [Index(2, \"site $i\") for i in 1:n_physical_indexes]\n",
    "\n",
    "# Define bond indices for each layer counting from the bottom\n",
    "n_bond_indexes_layer1 = div(n_physical_indexes, 2) # 8 bond indexes\n",
    "n_bond_indexes_layer2 = div(n_bond_indexes_layer1, 2)   # 4 bond indexes\n",
    "\n",
    "# Define bond indices\n",
    "bond_dimension = 2\n",
    "bond_indexes_layer1 = [Index(bond_dimension, \"layer1_bond $i\") for i in 1:n_bond_indexes_layer1]\n",
    "bond_indexes_layer2 = [Index(bond_dimension, \"layer2_bond $i\") for i in 1:n_bond_indexes_layer2]\n",
    "\n",
    "# Define tensors and normalize them\n",
    "layer1_tensors = [ randomITensor(physical_indexes[2*i-1], physical_indexes[2*i], bond_indexes_layer1[i]) for i in 1:n_bond_indexes_layer1 ]\n",
    "layer1_tensors = [ normalize!(t) for t in layer1_tensors ]\n",
    "layer2_tensors = [ randomITensor(bond_indexes_layer1[2*i-1], bond_indexes_layer1[2*i], bond_indexes_layer2[i]) for i in 1:n_bond_indexes_layer2 ]\n",
    "layer2_tensors = [ normalize!(t) for t in layer2_tensors ]\n",
    "root_tensor = randomITensor(bond_indexes_layer2[1], bond_indexes_layer2[2])\n",
    "root_tensor = normalize!(root_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTN converted to CUDA tensors\n"
     ]
    }
   ],
   "source": [
    "# Converting tensors to CUDA tensors\n",
    "layer1_tensors = [ cu(t) for t in layer1_tensors ]\n",
    "layer2_tensors = [ cu(t) for t in layer2_tensors ]\n",
    "root_tensor = cu(root_tensor)\n",
    "println(\"TTN converted to CUDA tensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contracting the TTN as a check of indexes naming\n",
    "result = root_tensor\n",
    "for t in layer2_tensors\n",
    "    result = result * t\n",
    "end\n",
    "for t in layer1_tensors\n",
    "    result = result * t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITensor ord=0\n",
      "NDTensors.Dense{Float64, Vector{Float64}}\n",
      " 0-dimensional\n",
      "1.0000001192092896"
     ]
    }
   ],
   "source": [
    "# Make a vector of all layers\n",
    "TTN = vcat(layer1_tensors, layer2_tensors, [root_tensor])\n",
    "print(squared_norm(TTN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonicalization basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify things, we need to bring our tensors in canonical form, in which each tensor is in canonical form for an index (in following case for index $\\alpha_2$)\n",
    "$$\n",
    "\\sum_{\\alpha_4, \\alpha_5} T_{\\alpha_2, \\alpha_4, \\alpha_5}^{[2]} T_{\\alpha_2^{\\prime}, \\alpha_4, \\alpha_5}^{[2]}=\\delta_{\\alpha_2, \\alpha_2^{\\prime}},\n",
    "$$\n",
    "\n",
    "We can do so, by using QR decomposition. Starting from the leaves, we can decompose each tensor in a Unitary part and a Residual part. Then propagating the residual part towards the root we can for a \"central tensor\" containing all non-canonical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norma di Q: 1.4142135623730951\n",
      "norma di R: 1.0\n"
     ]
    }
   ],
   "source": [
    "#= # QR decomposition of a tensor as example\n",
    "# Example of building a simple TTN manually\n",
    "nsites = 3\n",
    "sites = [Index(2, \"site $i\") for i in 1:nsites]\n",
    "\n",
    "A = randomITensor(sites[1], sites[2], sites[3])\n",
    "A = A / norm(A)\n",
    "Q, R = qr(A, [sites[1], sites[2]], [sites[3]])\n",
    "println(\"norma di Q: \", norm(Q))\n",
    "println(\"norma di R: \", norm(R)) =#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed tensor: ITensor ord=3\n",
      "Dim 1: (dim=2|id=94|\"site1\")\n",
      "Dim 2: (dim=2|id=475|\"site2\")\n",
      "Dim 3: (dim=2|id=880|\"site3\")\n",
      "NDTensors.Dense{Float64, Vector{Float64}}\n",
      " 2×2×2\n",
      "[:, :, 1] =\n",
      "  0.6369867870151104   0.3969227295013099\n",
      " -0.34269229258817324  0.19266989172896\n",
      "\n",
      "[:, :, 2] =\n",
      " -0.4463347575876417   0.01497464111510948\n",
      "  0.2836623309589739  -0.04729917211522243\n",
      "Reconstruction error: 8.457555311126093e-17\n"
     ]
    }
   ],
   "source": [
    "#= A_reconstructed = Q * R\n",
    "println(\"Reconstructed tensor: \", A_reconstructed)\n",
    "println(\"Reconstruction error: \", norm(A - A_reconstructed))\n",
    " =#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q * Q_dagger: ITensor ord=0\n",
      "NDTensors.Dense{Float64, Vector{Float64}}\n",
      " 0-dimensional\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "#= # Check that Q is unitary\n",
    "# Normalize Q\n",
    "Q = Q\n",
    "Q_dagger = dag(Q)\n",
    "result = Q * Q_dagger\n",
    "println(\"Q * Q_dagger: \", result) =#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonicalization of TTN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now procede in canonicalization procedure for each layer of the TNN, taking the root tensor as central tensor. In this procedure we have to keep an eye on indexes, since the QR decomposition, the first block of indices is assigned to Q and the second one (in our case, the only \"upper\" one) is assigned to R, to be contracted with the next layer. The algorithm creates a new set of indexes beetween the first and second layer, that we have to rename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Tuple{Index{Int64}, Index{Int64}}}:\n",
       " ((dim=2|id=977|\"layer2_bond1\"), (dim=2|id=431|\"layer2_bond2\"))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving indexes\n",
    "old_indexes_layer1 = [inds(A) for A in layer1_tensors]\n",
    "old_indexes_layer2 = [inds(A) for A in layer2_tensors]\n",
    "old_indexes_root = [inds(root_tensor)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting canonical conversion...\n",
      "Canonical conversion of layer 1\n",
      "decomposed tensor 1 in layer 1\n",
      "decomposed tensor 2 in layer 1\n",
      "decomposed tensor 3 in layer 1\n",
      "decomposed tensor 4 in layer 1\n"
     ]
    }
   ],
   "source": [
    "# Canonicalization algorithm:\n",
    "# osservazione: tutti i nodi dell'albero sono normalizzati, sto normalizzando tutti i Q e spostando la norma su R\n",
    "\n",
    "println(\"Starting canonical conversion...\")\n",
    "println(\"Canonical conversion of layer 1\")\n",
    "for i in 1:length(layer1_tensors)\n",
    "    A = layer1_tensors[i]\n",
    "    Q, R = qr(A, [physical_indexes[2*i-1], physical_indexes[2*i]], [bond_indexes_layer1[i]])\n",
    "    Q = Q / norm(Q)\n",
    "    R = R * norm(Q)\n",
    "    layer1_tensors[i] = Q\n",
    "    layer2_tensors[Int(ceil(i/2))] = layer2_tensors[Int(ceil(i/2))] * R\n",
    "    println(\"decomposed tensor $i in layer 1\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming indexes\n"
     ]
    }
   ],
   "source": [
    "# Renaming indexes\n",
    "println(\"Renaming indexes\")\n",
    "rename_tensor_indices!(layer1_tensors, old_indexes_layer1)\n",
    "rename_tensor_indices!(layer2_tensors, old_indexes_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canonical conversion of layer 2\n",
      "decomposed tensor 1 in layer 2\n",
      "decomposed tensor 2 in layer 2\n"
     ]
    }
   ],
   "source": [
    "println(\"Canonical conversion of layer 2\")\n",
    "for i in 1:length(layer2_tensors)\n",
    "    A = layer2_tensors[i]\n",
    "    Q, R = qr(A, [bond_indexes_layer1[i*2-1], bond_indexes_layer1[2*i]], [bond_indexes_layer2[i]])\n",
    "    Q = Q / norm(Q)\n",
    "    R = R * norm(Q)\n",
    "    layer2_tensors[i] = Q\n",
    "    root_tensor = root_tensor * R\n",
    "    println(\"decomposed tensor $i in layer 2\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename indexes of layer 2 tensors\n",
    "rename_tensor_indices!(layer2_tensors, old_indexes_layer2)\n",
    "rename_tensor_indices!(root_tensor, old_indexes_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#= println(\"Layer 1:\")\n",
    "print_tensor_vector(layer1_tensors, \"Layer 1 Tensors\")\n",
    "println(\"Layer 2:\")\n",
    "print_tensor_vector(layer2_tensors, \"Layer 2 Tensors\")\n",
    "println(\"Root:\")\n",
    "print_tensor_vector(root_tensor, \"Root Tensor\") =#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the squared norm of the entire TTN making it contract with its conjugate (function defined in TTN_utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITensor ord=0\n",
      "NDTensors.Dense{Float64, Vector{Float64}}\n",
      " 0-dimensional\n",
      "0.009564132429659061"
     ]
    }
   ],
   "source": [
    "# Make a vector of all layers\n",
    "TTN = vcat(layer1_tensors, layer2_tensors, [root_tensor])\n",
    "println(squared_norm(TTN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITensor ord=0\n",
      "NDTensors.Dense{Float32, CuArray{Float32, 1, CUDA.DeviceMemory}}\n",
      " 0-dimensional\n",
      "0.009564132f0"
     ]
    }
   ],
   "source": [
    "println(root_tensor*dag(root_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we wanted, the root is now the central tensor, and its squared norm equals to the one of the entire TTN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
