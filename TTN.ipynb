{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ITensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Index{Int64}}:\n",
       " (dim=2|id=23|\"site1\")\n",
       " (dim=2|id=157|\"site2\")\n",
       " (dim=2|id=846|\"site3\")\n",
       " (dim=2|id=259|\"site4\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of building a simple TTN manually\n",
    "nsites = 4\n",
    "sites = [Index(2, \"site $i\") for i in 1:nsites]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ITensors when we define an index, we are implicitly defining a dimension for a generic vector space. In this case we are definig 4 bidimensional spaces (for example each one can be associated with a spin).\n",
    "\n",
    "We can use indexes to define and work with tensors. For example defining: \n",
    "\n",
    "-- ITensor(index(2,a)) creates a rank-1 tensor of dimension 2, so a vector of lenght 2.\n",
    "\n",
    "-- ITensor(index(2,a), index*(2,a)) creates a 2x2 matrix, so a transformation between the two 2d spaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contraction is a sum over indexes with same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaf tensors\n",
    "leaf1 = randomITensor(sites[1], sites[2])\n",
    "leaf2 = randomITensor(sites[3], sites[4])\n",
    "\n",
    "# Parent tensor\n",
    "parent = randomITensor(sites[1], sites[2], sites[3], sites[4])\n",
    "\n",
    "# contracting\n",
    "result = parent * leaf1 * leaf2\n",
    "\n",
    "println(\"Result of contracting the TTN with random vectors: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABC = @visualize leaf1 * leaf2 * parent edge_labels=(tags=true,);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contracting parent, a ITensor(i1,i2,i3,i4) with the two leaves, ITensor(i1,i2) and ITensor(i3,i4) returns a scalar since we are summing over all indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-TensorNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Screenshot from 2025-01-13 10-35-28.png\" alt=\"Example Image\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated in Fig. 1(b), each circle represents\n",
    "a tensor; each edge of the circle represents an individual\n",
    "index of the tensor. The first tensor is a matrix connecting the\n",
    "second and third tensors, while the remaining tensors are all\n",
    "three-order tensors with three indices. The index between two\n",
    "tensors is called a virtual bond, which would be contracted\n",
    "hereafter. The left and right indices of the tensors in the\n",
    "bottom of the TTN are respectively connected to two pixels\n",
    "of the input image and hence are called physical bonds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bond dimension in a tensor network refers to the dimension of the shared index (or bond) connecting two tensors. \n",
    "Higher bond dimensions allow more complex entanglement between subsystems to be represented. In quantum many-body systems, the bond dimension represents the number of states used to approximate the entanglement structure.\n",
    "\n",
    "Since bond dimension represents on how many indices we are summing, it is related to how well we can represent states with our tensor network.\n",
    "In the context of Matrix Product States (MPS), the bond dimension directly corresponds to the Schmidt rank of the state for each bipartition. If the bond dimension is h , the MPS can capture states with a maximum Schmidt rank of h.\n",
    "\n",
    "$|\\psi\\rangle=\\sum_{i=1}^D \\lambda_i\\left|u_i\\right\\rangle_A \\otimes\\left|v_i\\right\\rangle_B$, where D is the Schmidt rank, the number of non-zero $\\lambda$, which quantifies the enganglement between the two subsystems A and B (for D=1 the state is separable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ITensor ord=2 (dim=2|id=816|\"bond1\") (dim=2|id=895|\"bond2\")\n",
       "NDTensors.Dense{Float64, Vector{Float64}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's make a TTN with 3 layers, consisting of rank-3 tensors with bond dimension 2 with random elements\n",
    "\n",
    "# Define physical indices\n",
    "n_physical_indexes = 8\n",
    "physical_indexes = [Index(2, \"site $i\") for i in 1:n_physical_indexes]\n",
    "\n",
    "# Define bond indices for each layer counting from the bottom\n",
    "n_bond_indexes_layer1 = div(n_physical_indexes, 2) # 8 bond indexes\n",
    "n_bond_indexes_layer2 = div(n_bond_indexes_layer1, 2)   # 4 bond indexes\n",
    "\n",
    "# Define bond indices\n",
    "bond_dimension = 2\n",
    "bond_indexes_layer1 = [Index(bond_dimension, \"bond $i\") for i in 1:n_bond_indexes_layer1]\n",
    "bond_indexes_layer2 = [Index(bond_dimension, \"bond $i\") for i in 1:n_bond_indexes_layer2]\n",
    "\n",
    "# Define tensors and normalize them\n",
    "layer1_tensors = [ randomITensor(physical_indexes[2*i-1], physical_indexes[2*i], bond_indexes_layer1[i]) for i in 1:n_bond_indexes_layer1 ]\n",
    "layer1_tensors = [ normalize!(t) for t in layer1_tensors ]\n",
    "layer2_tensors = [ randomITensor(bond_indexes_layer1[2*i-1], bond_indexes_layer1[2*i], bond_indexes_layer2[i]) for i in 1:n_bond_indexes_layer2 ]\n",
    "layer2_tensors = [ normalize!(t) for t in layer2_tensors ]\n",
    "root_tensor = randomITensor(bond_indexes_layer2[1], bond_indexes_layer2[2])\n",
    "root_tensor = normalize!(root_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of contracting the TTN: ITensor ord=8\n",
      "Dim 1: (dim=2|id=264|\"site1\")\n",
      "Dim 2: (dim=2|id=14|\"site2\")\n",
      "Dim 3: (dim=2|id=424|\"site3\")\n",
      "Dim 4: (dim=2|id=662|\"site4\")\n",
      "Dim 5: (dim=2|id=946|\"site5\")\n",
      "Dim 6: (dim=2|id=262|\"site6\")\n",
      "Dim 7: (dim=2|id=25|\"site7\")\n",
      "Dim 8: (dim=2|id=156|\"site8\")\n",
      "NDTensors.Diag{Float64, Float64}\n",
      " 2×2×2×2×2×2×2×2\n",
      "[:, :, 1, 1, 1, 1, 1, 1] =\n",
      " 1.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 1, 1, 1, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 1, 1, 1, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 1, 1, 1, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 1, 2, 1, 1, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 2, 1, 1, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 2, 1, 1, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 2, 1, 1, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 1, 1, 2, 1, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 1, 2, 1, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 1, 2, 1, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 1, 2, 1, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 1, 2, 2, 1, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 2, 2, 1, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 2, 2, 1, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 2, 2, 1, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 1, 1, 1, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 1, 1, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 1, 1, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 1, 1, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 1, 2, 1, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 2, 1, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 2, 1, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 2, 1, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 1, 1, 2, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 1, 2, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 1, 2, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 1, 2, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 1, 2, 2, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 2, 2, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 2, 2, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 2, 2, 2, 1] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 1, 1, 1, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 1, 1, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 1, 1, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 1, 1, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 1, 2, 1, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 2, 1, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 2, 1, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 2, 1, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 1, 1, 2, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 1, 2, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 1, 2, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 1, 2, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 1, 2, 2, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 2, 2, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 2, 2, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 2, 2, 1, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 1, 1, 1, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 1, 1, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 1, 1, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 1, 1, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 1, 2, 1, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 2, 1, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 2, 1, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 2, 1, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 1, 1, 2, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 1, 2, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 1, 2, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 1, 2, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 1, 2, 2, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 1, 2, 2, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 1, 2, 2, 2, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  0.0\n",
      "\n",
      "[:, :, 2, 2, 2, 2, 2, 2] =\n",
      " 0.0  0.0\n",
      " 0.0  1.0\n"
     ]
    }
   ],
   "source": [
    "# Contracting\n",
    "result = root_tensor\n",
    "for t in layer2_tensors\n",
    "    result = result * t\n",
    "end\n",
    "for t in layer1_tensors\n",
    "    result = result * t\n",
    "end\n",
    "println(\"Result of contracting the TTN: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonicalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify things, we need to bring our tensors in canonical form, in which each tensor is in canonical form for an index (in following case for index $\\alpha_2$)\n",
    "$$\n",
    "\\sum_{\\alpha_4, \\alpha_5} T_{\\alpha_2, \\alpha_4, \\alpha_5}^{[2]} T_{\\alpha_2^{\\prime}, \\alpha_4, \\alpha_5}^{[2]}=\\delta_{\\alpha_2, \\alpha_2^{\\prime}},\n",
    "$$\n",
    "\n",
    "We can do so, by using QR decomposition. Starting from the leaves, we can decompose each tensor in a Unitary part and a Residual part. Then propagating the residual part towards the root we can for a \"central tensor\" containing all non-canonical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norma di Q: 1.414213562373095\n",
      "norma di R: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# QR decomposition of a tensor as example\n",
    "# Example of building a simple TTN manually\n",
    "nsites = 3\n",
    "sites = [Index(2, \"site $i\") for i in 1:nsites]\n",
    "\n",
    "A = randomITensor(sites[1], sites[2], sites[3])\n",
    "A = A / norm(A)\n",
    "Q, R = qr(A, [sites[1], sites[2]], [sites[3]])\n",
    "println(\"norma di Q: \", norm(Q))\n",
    "println(\"norma di R: \", norm(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed tensor: ITensor ord=3\n",
      "Dim 1: (dim=2|id=969|\"site1\")\n",
      "Dim 2: (dim=2|id=200|\"site2\")\n",
      "Dim 3: (dim=2|id=208|\"site3\")\n",
      "NDTensors.Dense{Float64, Vector{Float64}}\n",
      " 2×2×2\n",
      "[:, :, 1] =\n",
      "  0.048089857450470674   0.7699331237566488\n",
      " -0.524060519759655     -0.19138062895103594\n",
      "\n",
      "[:, :, 2] =\n",
      " 0.1785123678816708   -0.10475232562243073\n",
      " 0.19817155433257738   0.10729723616035085\n",
      "Reconstruction error: 1.5639472591400866e-16\n"
     ]
    }
   ],
   "source": [
    "A_reconstructed = Q * R\n",
    "println(\"Reconstructed tensor: \", A_reconstructed)\n",
    "println(\"Reconstruction error: \", norm(A - A_reconstructed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q * Q_dagger: ITensor ord=0\n",
      "NDTensors.Dense{Float64, Vector{Float64}}\n",
      " 0-dimensional\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Check that Q is unitary\n",
    "# Normalize Q\n",
    "Q = Q\n",
    "Q_dagger = dag(Q)\n",
    "result = Q * Q_dagger\n",
    "println(\"Q * Q_dagger: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonicalization of TTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `layer1_tensors` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `layer1_tensors` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/tesi/code/GATN/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X31sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "A = layer1_tensors[1]\n",
    "Q, R = qr(A, [physical_indexes[1], physical_indexes[2]], [bond_indexes_layer1[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ITensor ord=3 (dim=2|id=624|\"site5\") (dim=2|id=745|\"site6\") (dim=2|id=436|\"Link,qr\")\n",
       "NDTensors.Dense{Float64, Vector{Float64}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer1_tensors[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting canonical conversion...\n",
      "Layer 1\n",
      "decomposed tensor 1 in layer 1\n",
      "decomposed tensor 2 in layer 1\n",
      "decomposed tensor 3 in layer 1\n",
      "decomposed tensor 4 in layer 1\n",
      "Layer 2\n",
      "ok\n"
     ]
    },
    {
     "ename": "ErrorException",
     "evalue": "In `permute(::ITensor, inds...)`, the input ITensor has indices: \n\n((dim=2|id=555|\"CMB,Link\"), (dim=2|id=219|\"Link,qr\"), (dim=2|id=70|\"Link,qr\"), (dim=1|id=730|\"CMB,Link\"))\n\nbut the desired Index ordering is: \n\n((dim=1|id=730|\"CMB,Link\"), (dim=2|id=555|\"CMB,Link\"))",
     "output_type": "error",
     "traceback": [
      "In `permute(::ITensor, inds...)`, the input ITensor has indices: \n",
      "\n",
      "((dim=2|id=555|\"CMB,Link\"), (dim=2|id=219|\"Link,qr\"), (dim=2|id=70|\"Link,qr\"), (dim=1|id=730|\"CMB,Link\"))\n",
      "\n",
      "but the desired Index ordering is: \n",
      "\n",
      "((dim=1|id=730|\"CMB,Link\"), (dim=2|id=555|\"CMB,Link\"))\n",
      "\n",
      "Stacktrace:\n",
      " [1] error(s::String)\n",
      "   @ Base ./error.jl:35\n",
      " [2] permute(::ITensor, ::Index{Int64}, ::Vararg{Index{Int64}}; kwargs::@Kwargs{allow_alias::Bool})\n",
      "   @ ITensors ~/.julia/packages/ITensors/KvHvB/src/tensor_operations/permutations.jl:40\n",
      " [3] qx(qx::typeof(qr), A::ITensor, Linds::Vector{Index{Int64}}, Rinds::Vector{Index{Int64}}; tags::ITensors.TagSets.GenericTagSet{BitIntegers.UInt256, 4}, positive::Bool)\n",
      "   @ ITensors ~/.julia/packages/ITensors/KvHvB/src/tensor_operations/matrix_decomposition.jl:511\n",
      " [4] qx\n",
      "   @ ~/.julia/packages/ITensors/KvHvB/src/tensor_operations/matrix_decomposition.jl:488 [inlined]\n",
      " [5] #qr#309\n",
      "   @ ~/.julia/packages/ITensors/KvHvB/src/tensor_operations/matrix_decomposition.jl:474 [inlined]\n",
      " [6] qr(A::ITensor, Linds::Vector{Index{Int64}}, Rinds::Vector{Index{Int64}})\n",
      "   @ ITensors ~/.julia/packages/ITensors/KvHvB/src/tensor_operations/matrix_decomposition.jl:473\n",
      " [7] top-level scope\n",
      "   @ ~/Documents/tesi/code/GATN/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X30sZmlsZQ==.jl:22"
     ]
    }
   ],
   "source": [
    "# Canonicalization algorithm:\n",
    "\n",
    "# osservazione: tutti i nodi dell'albero sono normalizzati, sto normalizzando tutti i Q e spostando la norma su R\n",
    "# ricordarsi di rieseguire la cella di definizione albero prima di questa\n",
    "\n",
    "println(\"Starting canonical conversion...\")\n",
    "println(\"Layer 1\")\n",
    "for i in 1:length(layer1_tensors)\n",
    "    A = layer1_tensors[i]\n",
    "    Q, R = qr(A, [physical_indexes[2*i-1], physical_indexes[2*i]], [bond_indexes_layer1[i]])\n",
    "    Q = Q / norm(Q)\n",
    "    R = R * norm(Q)\n",
    "    layer1_tensors[i] = Q\n",
    "    layer2_tensors[Int(ceil(i/2))] = layer2_tensors[Int(ceil(i/2))] * R\n",
    "    println(\"decomposed tensor $i in layer 1\")\n",
    "end\n",
    "\n",
    "println(\"Layer 2\")\n",
    "for i in 1:length(layer2_tensors)\n",
    "    A = layer2_tensors[i]\n",
    "    Q, R = qr(A, [bond_indexes_layer1[2*i-1], bond_indexes_layer1[2*i]], [bond_indexes_layer2[i]]) #errore qua\n",
    "    Q = Q / norm(Q)\n",
    "    R = R * norm(Q)\n",
    "    layer2_tensors[i] = Q\n",
    "    root_tensor = root_tensor * R\n",
    "    println(\"decomposed tensor $i in layer 2\")\n",
    "end\n",
    "\n",
    "println(\"Root tensor assumed as central tensor\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
